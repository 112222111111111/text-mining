{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Jieba最大概率法分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1, 2], 1: [2, 3], 2: [3, 4], 3: [4], 4: [5, 6], 5: [6], 6: [7], 7: [8, 10], 8: [9, 10], 9: [10], 10: [11, 12], 11: [12], 12: [13], 13: [14], 14: [15], 15: [16], 16: [17], 17: [18, 19], 18: [19], 19: [20, 21], 20: [21], 21: [22]}\n",
      "共同  创造  美好  的  新世纪  ——  二  ○  ○  一  年  新年  贺词  \n",
      "{0: [1], 1: [2, 6], 2: [3], 3: [4], 4: [5], 5: [6], 6: [7, 8, 9], 7: [8, 9], 8: [9], 9: [10, 11, 12, 13], 10: [11, 12, 13], 11: [12, 13], 12: [13], 13: [14], 14: [15], 15: [16], 16: [17, 18], 17: [18], 18: [19], 19: [20], 20: [21], 21: [22]}\n",
      "（  二○○○年  十二月  三十一日  ）  （  附  图片  1  张  ）  \n",
      "{0: [1, 2], 1: [2], 2: [3], 3: [4], 4: [5, 6], 5: [6], 6: [7], 7: [8], 8: [9, 10], 9: [10], 10: [11], 11: [12], 12: [14], 13: [14], 14: [15], 15: [16], 16: [17]}\n",
      "女士  们  ，  先生  们  ，  同志  们  ，  朋友  们  ：  \n",
      "{0: [1], 1: [2], 2: [3], 3: [4], 4: [5], 5: [6, 7], 6: [7], 7: [8, 9], 8: [9], 9: [10, 11], 10: [11], 11: [12, 13], 12: [13], 13: [14], 14: [15, 16], 15: [16], 16: [17, 18], 17: [18, 19], 18: [19, 20], 19: [20], 20: [21], 21: [23], 22: [23], 23: [24, 25], 24: [25], 25: [26, 27], 26: [27], 27: [28], 28: [29], 29: [30, 31], 30: [31], 31: [32], 32: [33, 34], 33: [35], 34: [35], 35: [36], 36: [37, 38], 37: [38, 39], 38: [39, 40], 39: [40, 41], 40: [41, 42], 41: [42], 42: [43], 43: [44], 44: [45, 46, 47], 45: [46], 46: [47], 47: [48, 49], 48: [49], 49: [51], 50: [51], 51: [52], 52: [53, 54], 53: [54], 54: [55, 56], 55: [56], 56: [57, 58], 57: [58], 58: [59], 59: [60], 60: [61, 62], 61: [62], 62: [63], 63: [64]}\n",
      "2  0  0  1  年  新年  钟声  即将  敲响  。  人类  社会  前进  的  航船  就要  驶入  2  1  世纪  的  新  航程  。  中国  人民  进入  了  向  现代化  建设  第三  步  战略  目标  迈进  的  新  征程  。  \n",
      "{0: [1], 1: [2, 3], 2: [3], 3: [4, 5, 7], 4: [5, 6], 5: [6, 7], 6: [7], 7: [8], 8: [9, 10], 9: [10], 10: [11], 11: [12], 12: [13], 13: [14, 15], 14: [15], 15: [16, 17], 16: [17], 17: [18, 19], 18: [19], 19: [20, 21], 20: [21], 21: [22, 23], 22: [23], 23: [24, 25], 24: [25], 25: [26], 26: [27, 28], 27: [28], 28: [29, 30], 29: [30], 30: [31, 32], 31: [32], 32: [33, 34], 33: [34], 34: [35], 35: [36, 37], 36: [37], 37: [38, 39, 40], 38: [39], 39: [40], 40: [41], 41: [42], 42: [43, 44], 43: [44], 44: [45, 46], 45: [46], 46: [47, 48], 47: [48], 48: [49], 49: [50], 50: [51, 52], 51: [52], 52: [53, 54], 53: [54], 54: [55, 56, 57], 55: [56], 56: [57], 57: [58, 59], 58: [59], 59: [60], 60: [61, 62], 61: [62], 62: [63, 64], 63: [64], 64: [65, 66, 67], 65: [66], 66: [67], 67: [68, 69], 68: [69], 69: [70], 70: [71, 72], 71: [72], 72: [73, 74], 73: [74], 74: [75], 75: [76, 77], 76: [77], 77: [78, 79], 78: [79], 79: [80], 80: [81], 81: [82, 83], 82: [83], 83: [84, 85], 84: [85], 85: [86], 86: [88], 87: [88], 88: [89], 89: [90], 90: [91, 92], 91: [92], 92: [93, 95], 93: [94, 95], 94: [95], 95: [97], 96: [97, 98], 97: [98], 98: [99, 100], 99: [100], 100: [101], 101: [102, 103], 102: [103], 103: [104], 104: [105]}\n",
      "在  这个  激动人心  的  时刻  ，  我  很  高兴  通过  中国  国际  广播  电台  、  中央  人民  广播  电台  和  中央  电视台  ，  向  全国  各族  人民  ，  向  香港  特别  行政区  同胞  、  澳门  特别  行政区  同胞  和  台湾  同胞  、  海外  侨胞  ，  向  世界  各国  的  朋友  们  ，  致以  新世纪  第一  个  新年  的  祝贺  ！  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "#import nltk\n",
    "#from nltk.corpus import PlaintextCorpusReader\n",
    " \n",
    "path1 = './icwb2-data-master/gold/pku_training_words.utf8'\n",
    "path2 = './icwb2-data-master/training/pku_training.utf8'\n",
    "path3 = './icwb2-data-master/testing/pku_test.utf8'\n",
    " \n",
    " \n",
    "with open(path1, 'rt', encoding='utf-8') as f:\n",
    "    training_words = [w.strip() for w in f.readlines()]\n",
    " \n",
    "#training = PlaintextCorpusReader( *os.path.split(path2) )\n",
    "#print(training)\n",
    "#training_words += list(training.words())\n",
    "\n",
    "with open(path2,'rt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        training_words += line.split('  ')\n",
    "N = len(training_words)\n",
    "V = len( set(training_words) )\n",
    "#fdist = nltk.FreqDist(training_words)\n",
    "fdist = {}\n",
    "for word in training_words:\n",
    "    if word not in fdist:\n",
    "        fdist[word]=1\n",
    "    else:\n",
    "        fdist[word]+=1\n",
    "fdist = dict([(w, math.log((c+1.0)/(N+V))) for w, c in fdist.items()])\n",
    "defprob = math.log(1.0/(N+V))\n",
    " \n",
    "with open(path3, 'rt', encoding='utf8') as f:\n",
    "    test = f.readlines()\n",
    " \n",
    "def get_DAG(sentence):\n",
    "    DAG = {}\n",
    "    T = len(sentence)\n",
    "    for x in range(T):\n",
    "        ys = []\n",
    "        for y in range(x+1, T+1):\n",
    "            if sentence[x:y] in fdist:\n",
    "                ys.append(y)\n",
    "        if not ys:\n",
    "            ys.append(x+1)\n",
    "        DAG[x] = ys\n",
    "    return DAG\n",
    "\n",
    "'''\n",
    "深度优先搜索\n",
    "'''\n",
    "def dfs(DAG, sentence):\n",
    "    segments = []\n",
    "    T = len(sentence)\n",
    "    def _dfs(words, x):\n",
    "        for y in DAG[x]:\n",
    "            if y < T:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                _dfs(new, y)\n",
    "            else:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                segments.append( new )\n",
    "    _dfs([], 0)\n",
    "    bestseg = max([(sum(fdist.get(w, defprob) for w in seg), seg) for seg in segments])\n",
    "    return bestseg[1]\n",
    "\n",
    "'''\n",
    "动态规划搜索最大权重\n",
    "'''\n",
    "def dp(DAG, sentence):\n",
    "    T = len(sentence)\n",
    "    prob = {T:(0.0,)}\n",
    "    for x in range(T-1, -1, -1):\n",
    "        prob[x] = max([(fdist.get(sentence[x:y], defprob) + prob[y][0], y) for y in DAG[x]])\n",
    "    x = 0\n",
    "    bestseg = []\n",
    "    while x < T:\n",
    "        y = prob[x][1]\n",
    "        bestseg.append( sentence[x:y] )\n",
    "        x = y\n",
    "    return bestseg\n",
    "\n",
    "'''\n",
    "分词测试\n",
    "'''\n",
    "for sent in test[:5]:\n",
    "    DAG = get_DAG(sent)\n",
    "    print(DAG)\n",
    "    #seg1 = dfs(DAG, sent)\n",
    "    seg2 = dp(DAG, sent)\n",
    "    #print('  '.join(seg1), sep='', end='')\n",
    "    print('  '.join(seg2), sep='', end='')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. jieba最大概率分词方法分步解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典中的最后10个词：\n",
      " ['鲁伦', '听筒', '供过于求', '牵挂', '照', '莺莺', '煤', '表皮', '煮', '宏愿']\n",
      "词典中的总词数：\n",
      " 55303\n"
     ]
    }
   ],
   "source": [
    "path1 = './icwb2-data-master/gold/pku_training_words.utf8'\n",
    "path2 = './icwb2-data-master/training/pku_training.utf8'\n",
    "path3 = './icwb2-data-master/testing/pku_test.utf8'\n",
    "\n",
    "'''\n",
    "读取词典\n",
    "'''\n",
    "with open(path1, 'rt', encoding='utf-8') as f:\n",
    "    training_words = [w.strip() for w in f.readlines()]\n",
    "print('词典中的最后10个词：\\n',training_words[-10:])\n",
    "print('词典中的总词数：\\n',len(training_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加入训练语料之后的词典中的最后10个词：\n",
      " ['，', '\\n', '才', '发觉', '已', '迷失', '了', '来路', '。', '\\n']\n",
      "加入训练语料之后的词典中的总词数：\n",
      " 1184306\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "将训练语料中的词加入词典\n",
    "'''\n",
    "with open(path2,'rt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        training_words += line.split('  ')\n",
    "print('加入训练语料之后的词典中的最后10个词：\\n',training_words[-10:])\n",
    "print('加入训练语料之后的词典中的总词数：\\n',len(training_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总词频：\n",
      " 1184306\n",
      "总词数：\n",
      " 55325\n"
     ]
    }
   ],
   "source": [
    "N = len(training_words)\n",
    "V = len( set(training_words) )\n",
    "print('总词频：\\n',N)\n",
    "print('总词数：\\n',V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55325\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "fdist = {}\n",
    "for word in training_words:\n",
    "    if word not in fdist:\n",
    "        fdist[word]=1\n",
    "    else:\n",
    "        fdist[word]+=1\n",
    "fdist = dict([(w, math.log((c+1.0)/(N+V))) for w, c in fdist.items()])\n",
    "defprob = math.log(1.0/(N+V))\n",
    "print(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path3, 'rt', encoding='utf8') as f:\n",
    "    test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DAG(sentence):\n",
    "    DAG = {}\n",
    "    T = len(sentence)\n",
    "    for x in range(T):\n",
    "        ys = []\n",
    "        for y in range(x+1, T+1):\n",
    "        #for y in range(x, T):\n",
    "            if sentence[x:y] in fdist:\n",
    "                ys.append(y)\n",
    "        if not ys:\n",
    "            ys.append(x+1)\n",
    "        DAG[x] = ys\n",
    "    return DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(DAG, sentence):\n",
    "    segments = []\n",
    "    T = len(sentence)\n",
    "    def _dfs(words, x):\n",
    "        for y in DAG[x]:\n",
    "            if y < T:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                _dfs(new, y)\n",
    "            else:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                segments.append( new )\n",
    "    _dfs([], 0)\n",
    "    bestseg = max([(sum(fdist.get(w, defprob) for w in seg), seg) for seg in segments])\n",
    "    return bestseg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp(DAG, sentence):\n",
    "    T = len(sentence)\n",
    "    prob = {T:(0.0,)}\n",
    "    for x in range(T-1, -1, -1):\n",
    "        prob[x] = max([(fdist.get(sentence[x:y], defprob) + prob[y][0], y) for y in DAG[x]])\n",
    "    x = 0\n",
    "    bestseg = []\n",
    "    while x < T:\n",
    "        y = prob[x][1]\n",
    "        bestseg.append( sentence[x:y] )\n",
    "        x = y\n",
    "    return bestseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去北京大学玩\n",
      "{0: [1], 1: [2, 3, 5], 2: [3], 3: [4, 5], 4: [5], 5: [6]}\n"
     ]
    }
   ],
   "source": [
    "sent = \"去北京大学玩\"\n",
    "print(sent)\n",
    "DAG = get_DAG(sent)\n",
    "print(DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"线程是程序执行时的最小单位，它是进程的一个执行流，\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009970664978027344\n",
      "线  程  是  程序  执行  时  的  最  小  单位  ，  它  是  进程  的  一个  执行  流  ，"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "DAG = get_DAG(sent)\n",
    "seg1 = dfs(DAG, sent)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print('  '.join(seg1), sep='', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009980201721191406\n",
      "线  程  是  程序  执行  时  的  最  小  单位  ，  它  是  进程  的  一个  执行  流  ，"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "DAG = get_DAG(sent)\n",
    "seg2 = dp(DAG, sent)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print('  '.join(seg2), sep='', end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HMM分词与序列标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共同  创造  美好  的  新  世纪  ——二○○一年  新年  贺词  \n",
      "（  二○○○年  十二月  三十  一日  ）  （  附  图片  1张  ）  \n",
      "女士  们  ，  先  生们  ，  同志  们  ，  朋友  们  ：  \n",
      "2001年  新年  钟声  即  将  敲响  。  人类  社会  前进  的  航船  就  要驶入21世纪  的  新  航程  。  中国  人民  进入  了  向  现代化  建设  第三步  战略  目标  迈进  的  新  征程  。  \n",
      "在  这个  激动  人心  的  时刻  ，  我  很  高兴  通过  中国  国际  广播  电台  、  中央  人民  广播  电台  和  中央  电视  台  ，  向  全国  各族  人民  ，  向  香港  特别  行政区  同胞  、  澳门  特别  行政区  同胞  和  台湾  同胞  、  海外  侨胞  ，  向  世界  各国  的  朋友  们  ，  致以  新  世纪  第一  个  新年  的  祝贺  ！  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys, re, math\n",
    " \n",
    "#sys.argv.append('./gold/pku_training_words.utf8')\n",
    "#sys.argv.append('./training/pku_training.utf8')\n",
    "#sys.argv.append('./testing/pku_test.utf8')\n",
    "\n",
    "training_words_filename = './icwb2-data-master/gold/pku_training_words.utf8'\n",
    "training_filename = './icwb2-data-master/training/pku_training.utf8'\n",
    "test_filename = './icwb2-data-master/testing/pku_test.utf8'\n",
    "\n",
    " \n",
    "with open(training_words_filename, 'rt', encoding='utf8') as f:\n",
    "    training_words = f.readlines()\n",
    " \n",
    "with open(training_filename, 'rt', encoding='utf8') as f:\n",
    "    training = f.readlines()\n",
    " \n",
    "with open(test_filename, 'rt', encoding='utf8') as f:\n",
    "    test = f.readlines()\n",
    " \n",
    "# word tag by char\n",
    "A, B, P = {}, {}, {}\n",
    "for line in training:\n",
    "    #print( line )\n",
    "    prev_a = None\n",
    "    for w, word in enumerate(re.split(r'\\s{2}', line)):\n",
    "        I = len(word)\n",
    "        for i, c in enumerate(word):\n",
    "            if I == 1:\n",
    "                a = 'S'\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    a = 'B'\n",
    "                elif i == I-1:\n",
    "                    a = 'E'\n",
    "                else:\n",
    "                    a = 'M'\n",
    "            # print(w, i, c, a)\n",
    "            if prev_a is None: # calculate Initial state Number\n",
    "                if a not in P: \n",
    "                    P[a] = 0\n",
    "                P[a] += 1\n",
    "            else: # calculate State transition Number\n",
    "                if prev_a not in A: \n",
    "                    A[prev_a] = {}\n",
    "                if a not in A[prev_a]: \n",
    "                    A[prev_a][a] = 0\n",
    "                A[prev_a][a] += 1\n",
    "            prev_a = a\n",
    "            # calculate Observation Number\n",
    "            if a not in B: \n",
    "                B[a] = {}\n",
    "            if c not in B[a]: \n",
    "                B[a][c] = 0\n",
    "            B[a][c] += 1\n",
    " \n",
    "# calculate probability\n",
    "for k, v in A.items():\n",
    "    total = sum(v.values())\n",
    "    A[k] = dict([(x, math.log(y / total)) for x, y in v.items()])\n",
    "\n",
    "for k, v in B.items():\n",
    "    # plus 1 smooth\n",
    "    total = sum(v.values())\n",
    "    V = len(v.values())\n",
    "    B[k] = dict([(x, math.log((y+1.0) / (total+V))) for x, y in v.items()])\n",
    "    # plus 1 smooth\n",
    "    B[k]['<UNK>'] = math.log(1.0 / (total+V))\n",
    "\n",
    "minlog = math.log( sys.float_info.min )\n",
    "total = sum(P.values())\n",
    "for k, v in P.items():\n",
    "    P[k] = math.log( v / total )\n",
    " \n",
    "def viterbi(observation):\n",
    "    #P(O|I,\\lambda)\n",
    "    state = ['B','M','E','S']\n",
    "    T = len(observation)\n",
    "    delta = [None] * (T + 1)\n",
    "    psi = [None] * (T + 1)\n",
    "    delta[0] = dict([(i, P.get(i, minlog) + B[i].get(observation[0], B[i]['<UNK>'])) for i in state])\n",
    "    psi[0] = dict([(i, None) for i in state])\n",
    "    for t in range(1, len(observation)):\n",
    "        Ot = observation[t]\n",
    "        delta[t] = dict([(j, max([delta[t-1][i] + A[i].get(j, minlog) + B[j].get(Ot, B[j]['<UNK>']) for i in state])) for j in state])\n",
    "        psi[t] = dict([(j, max([(delta[t-1][i] + A[i].get(j, minlog), i) for i in state])[1]) for j in state ])\n",
    "    delta[T] = max( [ delta[T-1][i] for i in state ] )\n",
    "    psi[T] = max([(delta[T-1][i], i) for i in state ] )[1]\n",
    "    q = [None] * (T+1)\n",
    "    q[T] = psi[T]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        q[t] = psi[t][q[t+1]]\n",
    "    return q[1:]\n",
    " \n",
    "for sent in test[:5]:\n",
    "    sequence = viterbi( list(sent) )\n",
    "    segment = []\n",
    "    for char, tag in zip(sent, sequence):\n",
    "        if tag == 'B':\n",
    "            segment.append(char)\n",
    "        elif tag == 'M':\n",
    "            segment[-1] += char\n",
    "        elif tag == 'E':\n",
    "            segment[-1] += char\n",
    "        elif tag == 'S':\n",
    "            segment.append(char)\n",
    "        else:\n",
    "            raise Exception()\n",
    "    print('  '.join(segment), sep='', end='')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "keywords by textrank:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\zhang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.781 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线程/\n",
      "进程/\n",
      "调度/\n",
      "单位/\n",
      "操作/\n",
      "请求/\n",
      "分配/\n",
      "允许/\n",
      "基本/\n",
      "并发/\n",
      "独立/\n",
      "资源/\n",
      "共享/\n",
      "执行/\n",
      "堆栈/\n",
      "分派/\n",
      "运行/\n",
      "实现/\n",
      "处理/\n",
      "程序执行/\n"
     ]
    }
   ],
   "source": [
    "from jieba import analyse\n",
    "# 引入TextRank关键词抽取接口\n",
    "textrank = analyse.textrank\n",
    " \n",
    "# 原始文本\n",
    "text = \"线程是程序执行时的最小单位，它是进程的一个执行流，\\\n",
    "        是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\\\n",
    "        线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\\\n",
    "        线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\\\n",
    "        同样多线程也可以实现并发操作，每个请求分配一个线程来处理。\"\n",
    " \n",
    "print(\"\\nkeywords by textrank:\")\n",
    "# 基于TextRank算法进行关键词抽取\n",
    "keywords = textrank(text)\n",
    "# 输出抽取出的关键词\n",
    "for keyword in keywords:\n",
    "    print(keyword + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "twisted 20.3.0 has requirement attrs>=19.2.0, but you'll have attrs 18.1.0 which is incompatible.\n",
      "automat 20.2.0 has requirement attrs>=19.2.0, but you'll have attrs 18.1.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords by tfidf:\n",
      "线程/\n",
      "CPU/\n",
      "进程/\n",
      "调度/\n",
      "多线程/\n",
      "程序执行/\n",
      "每个/\n",
      "执行/\n",
      "堆栈/\n",
      "局部变量/\n",
      "单位/\n",
      "并发/\n",
      "分派/\n",
      "一个/\n",
      "共享/\n",
      "请求/\n",
      "最小/\n",
      "可以/\n",
      "允许/\n",
      "分配/\n"
     ]
    }
   ],
   "source": [
    "from jieba import analyse\n",
    "# 引入TF-IDF关键词抽取接口\n",
    "tfidf = analyse.extract_tags\n",
    " \n",
    "# 原始文本\n",
    "text = \"线程是程序执行时的最小单位，它是进程的一个执行流，\\\n",
    "        是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\\\n",
    "        线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\\\n",
    "        线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\\\n",
    "        同样多线程也可以实现并发操作，每个请求分配一个线程来处理。\"\n",
    " \n",
    "# 基于TF-IDF算法进行关键词抽取\n",
    "keywords = tfidf(text)\n",
    "print(\"keywords by tfidf:\")\n",
    "# 输出抽取出的关键词\n",
    "for keyword in keywords:\n",
    "    print(keyword + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF\n",
    "TextRanK\n",
    "HMM-POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKUseg\n",
    "Thu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
