{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 双向最大匹配分词方法实现\n",
    "class BiMM():\n",
    "    def __init__(self, user_dict):\n",
    "        self.user_dict = user_dict\n",
    "    def cut(self, sentence):\n",
    "        \"\"\"\n",
    "        正向最大匹配（FMM）\n",
    "        :param user_dict: 词典\n",
    "        :param sentence: 句子\n",
    "        \"\"\"\n",
    "        # 词典中最长词长度\n",
    "        max_len = max([len(item) for item in self.user_dict])\n",
    "        start = 0\n",
    "        f_single_word_num = 0 #单字词数量计数\n",
    "        f_segs = [] #存储分词结果\n",
    "        while start != len(sentence):\n",
    "            index = start+max_len\n",
    "            if index>len(sentence):\n",
    "                index = len(sentence)\n",
    "            for i in range(max_len):\n",
    "                if (sentence[start:index] in self.user_dict) or (len(sentence[start:index])==1):\n",
    "                    f_segs.append(sentence[start:index])\n",
    "                    if index-start==1:\n",
    "                        f_single_word_num+=1\n",
    "                    #print(sentence[start:index], end='/')\n",
    "                    start = index\n",
    "                    break\n",
    "                index += -1\n",
    "        \"\"\"\n",
    "        反向最大匹配（BMM）\n",
    "        :param user_dict:词典\n",
    "        :param sentence:句子\n",
    "        \"\"\"\n",
    "        result = [] #暂存分词结果\n",
    "        b_segs = [] #存储分词结果\n",
    "        b_single_word_num = 0 #单字词数量计数\n",
    "        start = len(sentence)\n",
    "        while start != 0:\n",
    "            index = start - max_len\n",
    "            if index < 0:\n",
    "                index = 0\n",
    "            for i in range(max_len):\n",
    "                #print(sentence[index:start])\n",
    "                if (sentence[index:start] in self.user_dict) or (len(sentence[index:start])==1):\n",
    "                    result.append(sentence[index:start])\n",
    "                    if start-index == 1:\n",
    "                        b_single_word_num+=1\n",
    "                    start = index\n",
    "                    break\n",
    "                index += 1\n",
    "        b_segs = result[::-1]\n",
    "    \n",
    "        \"\"\"\n",
    "        双向最大匹配\n",
    "        \"\"\"\n",
    "        if len(f_segs)<len(b_segs):\n",
    "            return f_segs\n",
    "        elif len(f_segs)>len(b_segs):\n",
    "            return b_segs\n",
    "        else:\n",
    "            if f_single_word_num>b_single_word_num:\n",
    "                return b_segs\n",
    "            else:\n",
    "                return f_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '在', '野生动物园', '玩']\n"
     ]
    }
   ],
   "source": [
    "#测试双向最大匹配分词方法\n",
    "user_dict = ['我们','在', '在野', '生动', '野生', '动物园', '野生动物园', '物','园','玩']\n",
    "sentence = '我们在野生动物园玩'\n",
    "bimm = BiMM(user_dict)\n",
    "print(bimm.cut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从语料中提取的词典规模：\n",
      " 56482 47 22111\n"
     ]
    }
   ],
   "source": [
    "#从人民日报分词语料中读取分词数据，并构建词典\n",
    "#将人民日报分词语料还原成原文\n",
    "ribao_dict = [] #语料中出现的所有词\n",
    "ribao_dict_sorted = [] #过滤掉低频词后的词典\n",
    "yuanwen = [] #存储语料原文\n",
    "punctuation = []  #存储标点符号\n",
    "yuanwen_segged = []\n",
    "c = 0\n",
    "with open(\"199801.txt\",\"r\",encoding=\"utf-8\") as fr, open(\"199801_source.txt\",\"w\",encoding=\"utf-8\") as fw:\n",
    "    for line in fr:\n",
    "        #if c > 100:\n",
    "        #    break\n",
    "        ls = line.strip().split('  ')\n",
    "        line_segs = []\n",
    "        for i in range(1,len(ls)):\n",
    "            #print(ls[i])\n",
    "            #if '/' in ls[i]:\n",
    "            end_idx = ls[i].index('/')\n",
    "            #line_segs.append(ls[i][:end_idx])\n",
    "            ribao_dict.append(ls[i][:end_idx])\n",
    "        \n",
    "            if ls[i].endswith('/w'):\n",
    "                punctuation.append(ls[i][:end_idx])\n",
    "        yuanwen_segged.append(line_segs)\n",
    "        fw.write(''.join(line_segs)+'\\n')\n",
    "\n",
    "        yuanwen.append(''.join(line_segs))\n",
    "        #c+=1\n",
    "word_count_dict = {}\n",
    "for word in ribao_dict:\n",
    "    if word not in word_count_dict:\n",
    "        word_count_dict[word]=1\n",
    "    else:\n",
    "        word_count_dict[word]+=1\n",
    "sorted_word_count = sorted(word_count_dict.items(), key = lambda item:item[1], reverse=True)\n",
    "\n",
    "#保存出现频次大于2的词，存入词典\n",
    "for k,v in sorted_word_count:\n",
    "    if v>2:\n",
    "        ribao_dict_sorted.append(k)\n",
    "ribao_dict_sorted = set(ribao_dict_sorted)\n",
    "punctuation = set(punctuation)\n",
    "ribao_dict_set = set(ribao_dict)\n",
    "print(\"从语料中提取的词典规模：\\n\",len(ribao_dict_set),len(punctuation), len(ribao_dict_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['迈向', '新', '世纪']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用新词典测试双向最大匹配分词方法\n",
    "bimm = BiMM(ribao_dict_sorted)\n",
    "bimm.cut(\"迈向新世纪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segging: 100%|██████████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:06<00:00, 29.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共花费 66.9113290309906 秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm #进度条线上模块\n",
    "\n",
    "time_start = time.time() #计时\n",
    "source_text = []\n",
    "c = 0\n",
    "with open(\"199801_source.txt\",\"r\",encoding=\"utf-8\") as fr:\n",
    "    for line in fr:\n",
    "        c+=1\n",
    "        if c>2000:\n",
    "            break\n",
    "        source_text.append(line.strip())\n",
    "\n",
    "bimm = BiMM(ribao_dict_sorted)\n",
    "all_Bi_segs = [] #存储整个语料的分词结果\n",
    "with open(\"199801_source_segs.txt\",\"w\",encoding=\"utf-8\") as fw:\n",
    "    for t in tqdm(range(len(source_text)),desc = 'segging'):\n",
    "        text = source_text[t]  \n",
    "        start_idx = 0\n",
    "        Bi_segs = []\n",
    "        flag = True\n",
    "        #start_idx = 0\n",
    "        for idx in range(len(text)):\n",
    "            if text[idx] in punctuation: #有标点符号的，先根据标点符号节分句子\n",
    "                Bi_segs.extend(bimm.cut(text[start_idx:idx+1]))\n",
    "                start_idx = idx+1\n",
    "                flag = False\n",
    "        if flag:\n",
    "            Bi_segs.extend(bimm.cut(text))\n",
    "        else:\n",
    "            Bi_segs.extend(bimm.cut(text[start_idx:idx+1]))\n",
    "        all_Bi_segs.append(Bi_segs)\n",
    "        fw.write('  '.join(Bi_segs)+'\\n')\n",
    "time_end = time.time()\n",
    "print('共花费',time_end-time_start,\"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算分词准确率与召回率\n",
    "#words_num = 0\n",
    "def countWordNum(xis, yis):\n",
    "    line_text = ''.join(xis)\n",
    "    #print(line_text)\n",
    "    xis_idx_list = []\n",
    "    yis_idx_list = []\n",
    "    idx = 0\n",
    "    for xi in xis:\n",
    "        eidx = line_text.find(xi,idx)\n",
    "        xis_idx_list.append(eidx)\n",
    "        idx=eidx+1\n",
    "    idx = 0\n",
    "    for yi in yis:\n",
    "        eidx = line_text.find(yi,idx)\n",
    "        yis_idx_list.append(eidx)\n",
    "        idx=eidx+1\n",
    "    #print(xis_idx_list)\n",
    "    #print(yis_idx_list)\n",
    "    correct_count = 0\n",
    "    gold_word_count = len(xis_idx_list)-1\n",
    "    cut_word_count = len(yis_idx_list)-1\n",
    "    i = 1\n",
    "    j = 1\n",
    "    flag = True\n",
    "    \n",
    "    while(i<len(xis_idx_list) and j<len(yis_idx_list)):\n",
    "        if xis_idx_list[i]== yis_idx_list[j]:\n",
    "            if flag:\n",
    "                correct_count+=1\n",
    "            flag=True\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif xis_idx_list[i] < yis_idx_list[j]:\n",
    "            i+=1\n",
    "            flag=False\n",
    "        else:\n",
    "            j+=1\n",
    "            flag=False\n",
    "    \n",
    "    #print(xis_idx_list)\n",
    "    #print(yis_idx_list)\n",
    "    return correct_count, gold_word_count, cut_word_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "Accuracy: 0.8869\n",
      " Recall: 0.9398\n",
      " F1: 0.9126\n"
     ]
    }
   ],
   "source": [
    "with open(\"199801_top_2000.txt\",\"r\",encoding=\"utf-8\") as fr1, open(\"199801_source_top2000_segs.txt\",\"r\",encoding=\"utf-8\") as fr2:\n",
    "    total_correct_num = 0\n",
    "    total_gold_num = 0\n",
    "    total_cut_num = 0\n",
    "    c=0\n",
    "    while True:\n",
    "        #if c>1:\n",
    "        #    break\n",
    "        if c%1000==0:\n",
    "            print(c)\n",
    "        try:\n",
    "            c+=1\n",
    "            x = next(fr1)\n",
    "            y = next(fr2)\n",
    "            #print(\"打印分词文本：\\n\")\n",
    "            #print(x)\n",
    "            #print(y)\n",
    "            xis = []\n",
    "            yis = []\n",
    "            for xi in x.strip().split(\"  \")[1:]:\n",
    "                #print(xi)\n",
    "                e_idx = xi.index('/')\n",
    "                xis.append(xi[:e_idx])\n",
    "            for yi in y.strip().split(\"  \"):\n",
    "                #print(yi)\n",
    "                if '/' in yi:\n",
    "                    e_idx = yi.index('/')\n",
    "                    yis.append(yi[:e_idx])\n",
    "                else:\n",
    "                    yis.append(yi)\n",
    "            #xis = [xi[:xi.index('/')] for xi in x.split(\"  \")]\n",
    "            #yis = [yi[:yi.index('/')] for yi in y.split(\"  \")]\n",
    "            #print(xis)\n",
    "            #print(yis)\n",
    "            correct_num, gold_num, cut_num = countWordNum(xis, yis)\n",
    "            total_correct_num += correct_num\n",
    "            total_gold_num += gold_num\n",
    "            total_cut_num += cut_num\n",
    "            #TT+=TTi\n",
    "        except StopIteration:\n",
    "            break\n",
    "    accuracy = total_correct_num/total_cut_num\n",
    "    recall = total_correct_num/total_gold_num\n",
    "    f1 = 2*accuracy*recall/(accuracy+recall)\n",
    "    print(\"Accuracy: %.4f\\n Recall: %.4f\\n F1: %.4f\" % (accuracy, recall, f1))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大概率法分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有向无环图： {0: [1, 2], 1: [2, 3], 2: [3, 4], 3: [4], 4: [5, 6], 5: [6], 6: [7], 7: [8, 10], 8: [9, 10], 9: [10], 10: [11, 12], 11: [12], 12: [13], 13: [14], 14: [15], 15: [16], 16: [17], 17: [18, 19], 18: [19], 19: [20, 21], 20: [21], 21: [22]}\n",
      "共同  创造  美好  的  新世纪  ——  二  ○  ○  一  年  新年  贺词  \n",
      "有向无环图： {0: [1], 1: [2, 6], 2: [3], 3: [4], 4: [5], 5: [6], 6: [7, 8, 9], 7: [8, 9], 8: [9], 9: [10, 11, 12, 13], 10: [11, 12, 13], 11: [12, 13], 12: [13], 13: [14], 14: [15], 15: [16], 16: [17, 18], 17: [18], 18: [19], 19: [20], 20: [21], 21: [22]}\n",
      "（  二○○○年  十二月  三十一日  ）  （  附  图片  1  张  ）  \n",
      "有向无环图： {0: [1, 2], 1: [2], 2: [3], 3: [4], 4: [5, 6], 5: [6], 6: [7], 7: [8], 8: [9, 10], 9: [10], 10: [11], 11: [12], 12: [14], 13: [14], 14: [15], 15: [16], 16: [17]}\n",
      "女士  们  ，  先生  们  ，  同志  们  ，  朋友  们  ：  \n",
      "有向无环图： {0: [1], 1: [2], 2: [3], 3: [4], 4: [5], 5: [6, 7], 6: [7], 7: [8, 9], 8: [9], 9: [10, 11], 10: [11], 11: [12, 13], 12: [13], 13: [14], 14: [15, 16], 15: [16], 16: [17, 18], 17: [18, 19], 18: [19, 20], 19: [20], 20: [21], 21: [23], 22: [23], 23: [24, 25], 24: [25], 25: [26, 27], 26: [27], 27: [28], 28: [29], 29: [30, 31], 30: [31], 31: [32], 32: [33, 34], 33: [35], 34: [35], 35: [36], 36: [37, 38], 37: [38, 39], 38: [39, 40], 39: [40, 41], 40: [41, 42], 41: [42], 42: [43], 43: [44], 44: [45, 46, 47], 45: [46], 46: [47], 47: [48, 49], 48: [49], 49: [51], 50: [51], 51: [52], 52: [53, 54], 53: [54], 54: [55, 56], 55: [56], 56: [57, 58], 57: [58], 58: [59], 59: [60], 60: [61, 62], 61: [62], 62: [63], 63: [64]}\n",
      "2  0  0  1  年  新年  钟声  即将  敲响  。  人类  社会  前进  的  航船  就要  驶入  2  1  世纪  的  新  航程  。  中国  人民  进入  了  向  现代化  建设  第三  步  战略  目标  迈进  的  新  征程  。  \n",
      "有向无环图： {0: [1], 1: [2, 3], 2: [3], 3: [4, 5, 7], 4: [5, 6], 5: [6, 7], 6: [7], 7: [8], 8: [9, 10], 9: [10], 10: [11], 11: [12], 12: [13], 13: [14, 15], 14: [15], 15: [16, 17], 16: [17], 17: [18, 19], 18: [19], 19: [20, 21], 20: [21], 21: [22, 23], 22: [23], 23: [24, 25], 24: [25], 25: [26], 26: [27, 28], 27: [28], 28: [29, 30], 29: [30], 30: [31, 32], 31: [32], 32: [33, 34], 33: [34], 34: [35], 35: [36, 37], 36: [37], 37: [38, 39, 40], 38: [39], 39: [40], 40: [41], 41: [42], 42: [43, 44], 43: [44], 44: [45, 46], 45: [46], 46: [47, 48], 47: [48], 48: [49], 49: [50], 50: [51, 52], 51: [52], 52: [53, 54], 53: [54], 54: [55, 56, 57], 55: [56], 56: [57], 57: [58, 59], 58: [59], 59: [60], 60: [61, 62], 61: [62], 62: [63, 64], 63: [64], 64: [65, 66, 67], 65: [66], 66: [67], 67: [68, 69], 68: [69], 69: [70], 70: [71, 72], 71: [72], 72: [73, 74], 73: [74], 74: [75], 75: [76, 77], 76: [77], 77: [78, 79], 78: [79], 79: [80], 80: [81], 81: [82, 83], 82: [83], 83: [84, 85], 84: [85], 85: [86], 86: [88], 87: [88], 88: [89], 89: [90], 90: [91, 92], 91: [92], 92: [93, 95], 93: [94, 95], 94: [95], 95: [97], 96: [97, 98], 97: [98], 98: [99, 100], 99: [100], 100: [101], 101: [102, 103], 102: [103], 103: [104], 104: [105]}\n",
      "在  这个  激动人心  的  时刻  ，  我  很  高兴  通过  中国  国际  广播  电台  、  中央  人民  广播  电台  和  中央  电视台  ，  向  全国  各族  人民  ，  向  香港  特别  行政区  同胞  、  澳门  特别  行政区  同胞  和  台湾  同胞  、  海外  侨胞  ，  向  世界  各国  的  朋友  们  ，  致以  新世纪  第一  个  新年  的  祝贺  ！  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "#import nltk\n",
    "#from nltk.corpus import PlaintextCorpusReader\n",
    " \n",
    "path1 = './icwb2-data-master/gold/pku_training_words.utf8'\n",
    "path2 = './icwb2-data-master/training/pku_training.utf8'\n",
    "path3 = './icwb2-data-master/testing/pku_test.utf8'\n",
    " \n",
    " \n",
    "with open(path1, 'rt', encoding='utf-8') as f:\n",
    "    training_words = [w.strip() for w in f.readlines()]\n",
    " \n",
    "#training = PlaintextCorpusReader( *os.path.split(path2) )\n",
    "#print(training)\n",
    "#training_words += list(training.words())\n",
    "\n",
    "with open(path2,'rt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        training_words += line.split('  ')\n",
    "N = len(training_words)\n",
    "V = len( set(training_words) )\n",
    "#fdist = nltk.FreqDist(training_words)\n",
    "fdist = {}\n",
    "for word in training_words:\n",
    "    if word not in fdist:\n",
    "        fdist[word]=1\n",
    "    else:\n",
    "        fdist[word]+=1\n",
    "fdist = dict([(w, math.log((c+1.0)/(N+V))) for w, c in fdist.items()])\n",
    "defprob = math.log(1.0/(N+V))\n",
    " \n",
    "with open(path3, 'rt', encoding='utf8') as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "#DAG, Directed Acyclic Graph, 有向无环图\n",
    "def get_DAG(sentence):\n",
    "    DAG = {}\n",
    "    T = len(sentence)\n",
    "    for x in range(T):\n",
    "        ys = []\n",
    "        for y in range(x+1, T+1):\n",
    "            if sentence[x:y] in fdist:\n",
    "                ys.append(y)\n",
    "        if not ys:\n",
    "            ys.append(x+1)\n",
    "        DAG[x] = ys\n",
    "    return DAG\n",
    "\n",
    "#深度优先遍历获取最大权重的路径\n",
    "def dfs(DAG, sentence):\n",
    "    segments = []\n",
    "    T = len(sentence)\n",
    "    def _dfs(words, x):\n",
    "        for y in DAG[x]:\n",
    "            if y < T:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                _dfs(new, y)\n",
    "            else:\n",
    "                new = words.copy()\n",
    "                new.append(sentence[x:y])\n",
    "                segments.append( new )\n",
    "    _dfs([], 0)\n",
    "    bestseg = max([(sum(fdist.get(w, defprob) for w in seg), seg) for seg in segments])\n",
    "    return bestseg[1]\n",
    " \n",
    "def dp(DAG, sentence):\n",
    "    T = len(sentence)\n",
    "    prob = {T:(0.0,)}\n",
    "    for x in range(T-1, -1, -1):\n",
    "        prob[x] = max([(fdist.get(sentence[x:y], defprob) + prob[y][0], y) for y in DAG[x]])\n",
    "    x = 0\n",
    "    bestseg = []\n",
    "    while x < T:\n",
    "        y = prob[x][1]\n",
    "        bestseg.append( sentence[x:y] )\n",
    "        x = y\n",
    "    return bestseg\n",
    " \n",
    "for sent in test[:5]:\n",
    "    DAG = get_DAG(sent)\n",
    "    print(\"有向无环图：\",DAG)\n",
    "    #seg1 = dfs(DAG, sent)\n",
    "    seg2 = dp(DAG, sent)\n",
    "    #print('  '.join(seg1), sep='', end='')\n",
    "    print('  '.join(seg2), sep='', end='')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HMM分词与序列标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共同  创造  美好  的  新  世纪  ——二○○一年  新年  贺词  \n",
      "（  二○○○年  十二月  三十  一日  ）  （  附  图片  1张  ）  \n",
      "女士  们  ，  先  生们  ，  同志  们  ，  朋友  们  ：  \n",
      "2001年  新年  钟声  即  将  敲响  。  人类  社会  前进  的  航船  就  要驶入21世纪  的  新  航程  。  中国  人民  进入  了  向  现代化  建设  第三步  战略  目标  迈进  的  新  征程  。  \n",
      "在  这个  激动  人心  的  时刻  ，  我  很  高兴  通过  中国  国际  广播  电台  、  中央  人民  广播  电台  和  中央  电视  台  ，  向  全国  各族  人民  ，  向  香港  特别  行政区  同胞  、  澳门  特别  行政区  同胞  和  台湾  同胞  、  海外  侨胞  ，  向  世界  各国  的  朋友  们  ，  致以  新  世纪  第一  个  新年  的  祝贺  ！  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys, re, math\n",
    " \n",
    "#sys.argv.append('./gold/pku_training_words.utf8')\n",
    "#sys.argv.append('./training/pku_training.utf8')\n",
    "#sys.argv.append('./testing/pku_test.utf8')\n",
    "\n",
    "training_words_filename = './icwb2-data-master/gold/pku_training_words.utf8'\n",
    "training_filename = './icwb2-data-master/training/pku_training.utf8'\n",
    "test_filename = './icwb2-data-master/testing/pku_test.utf8'\n",
    "\n",
    " \n",
    "with open(training_words_filename, 'rt', encoding='utf8') as f:\n",
    "    training_words = f.readlines()\n",
    " \n",
    "with open(training_filename, 'rt', encoding='utf8') as f:\n",
    "    training = f.readlines()\n",
    " \n",
    "with open(test_filename, 'rt', encoding='utf8') as f:\n",
    "    test = f.readlines()\n",
    " \n",
    "# word tag by char\n",
    "A, B, P = {}, {}, {}\n",
    "for line in training:\n",
    "    #print( line )\n",
    "    prev_a = None\n",
    "    for w, word in enumerate(re.split(r'\\s{2}', line)):\n",
    "        I = len(word)\n",
    "        for i, c in enumerate(word):\n",
    "            if I == 1:\n",
    "                a = 'S'\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    a = 'B'\n",
    "                elif i == I-1:\n",
    "                    a = 'E'\n",
    "                else:\n",
    "                    a = 'M'\n",
    "            # print(w, i, c, a)\n",
    "            if prev_a is None: # calculate Initial state Number\n",
    "                if a not in P: \n",
    "                    P[a] = 0\n",
    "                P[a] += 1\n",
    "            else: # calculate State transition Number\n",
    "                if prev_a not in A: \n",
    "                    A[prev_a] = {}\n",
    "                if a not in A[prev_a]: \n",
    "                    A[prev_a][a] = 0\n",
    "                A[prev_a][a] += 1\n",
    "            prev_a = a\n",
    "            # calculate Observation Number\n",
    "            if a not in B: \n",
    "                B[a] = {}\n",
    "            if c not in B[a]: \n",
    "                B[a][c] = 0\n",
    "            B[a][c] += 1\n",
    " \n",
    "# calculate probability\n",
    "for k, v in A.items():\n",
    "    total = sum(v.values())\n",
    "    A[k] = dict([(x, math.log(y / total)) for x, y in v.items()])\n",
    "\n",
    "for k, v in B.items():\n",
    "    # plus 1 smooth\n",
    "    total = sum(v.values())\n",
    "    V = len(v.values())\n",
    "    B[k] = dict([(x, math.log((y+1.0) / (total+V))) for x, y in v.items()])\n",
    "    # plus 1 smooth\n",
    "    B[k]['<UNK>'] = math.log(1.0 / (total+V))\n",
    "\n",
    "minlog = math.log( sys.float_info.min )\n",
    "total = sum(P.values())\n",
    "for k, v in P.items():\n",
    "    P[k] = math.log( v / total )\n",
    " \n",
    "def viterbi(observation):\n",
    "    state = ['B','M','E','S']\n",
    "    T = len(observation)\n",
    "    delta = [None] * (T + 1)\n",
    "    psi = [None] * (T + 1)\n",
    "    delta[0] = dict([(i, P.get(i, minlog) + B[i].get(observation[0], B[i]['<UNK>'])) for i in state])\n",
    "    psi[0] = dict([(i, None) for i in state])\n",
    "    for t in range(1, len(observation)):\n",
    "        Ot = observation[t]\n",
    "        delta[t] = dict([(j, max([delta[t-1][i] + A[i].get(j, minlog) + B[j].get(Ot, B[j]['<UNK>']) for i in state])) for j in state])\n",
    "        psi[t] = dict([(j, max([(delta[t-1][i] + A[i].get(j, minlog), i) for i in state])[1]) for j in state ])\n",
    "    delta[T] = max( [ delta[T-1][i] for i in state ] )\n",
    "    psi[T] = max([(delta[T-1][i], i) for i in state ] )[1]\n",
    "    q = [None] * (T+1)\n",
    "    q[T] = psi[T]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        q[t] = psi[t][q[t+1]]\n",
    "    return q[1:]\n",
    " \n",
    "for sent in test[:5]:\n",
    "    sequence = viterbi( list(sent) )\n",
    "    segment = []\n",
    "    for char, tag in zip(sent, sequence):\n",
    "        if tag == 'B':\n",
    "            segment.append(char)\n",
    "        elif tag == 'M':\n",
    "            segment[-1] += char\n",
    "        elif tag == 'E':\n",
    "            segment[-1] += char\n",
    "        elif tag == 'S':\n",
    "            segment.append(char)\n",
    "        else:\n",
    "            raise Exception()\n",
    "    print('  '.join(segment), sep='', end='')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
